---
title: "Statistical Learning"
author: "Christophe Bontemps & Patrick Jonsson - SIAP"
subtitle: "Workshop on Machine Learning for Official Statistics in Asia-Pacific"
output:
  pdf_document:
    df_print: kable
    toc: true
    keep_tex: true
    fig_width: 6.5
    fig_height: 4
  word_document:
    toc: true
  html_document:
    df_print: paged
    toc: true
    keep_md: true
    code_folding: show
    toc_float: true
    fig_width: 6.5
    fig_height: 4
always_allow_html: true
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Knitr_Global_Options, include=FALSE, cache=FALSE}
library(knitr)

opts_chunk$set(warning = FALSE, message = FALSE, 
               autodep = TRUE, tidy = FALSE, cache = TRUE)
#opts_chunk$set(cache.rebuild=TRUE) 

# My colors:
SIAP.color <- "#0385a8"

```

`r if(knitr:::pandoc_to() == "latex") {paste("\\large")}` 

## Tools

We will use several packages to make our lives easier. Here is the list of packages we will use: 

```{r}
# Data Science packages
library(tidyverse)
library(jtools)

# Pretty tables
library(broom)
library(xtable)
library(kableExtra)

```

# Introduction
This file serves as support for the "*Workshop on Machine Learning for Official Statistics in Asia-Pacific*"  and showcase a typical work stream in Data Science.   


The term *machine learning* is used because the computer (more accurately the algorithm) figures out the model from the data and will then predict with accuracy. On the other hand,  *statistical learning* is mostly used to test whether a model fits the data and to highlight relationships between variables. 

> Let's start with a Data Science example

One specific feature is that in most cases the data is  "big":

- usually $n$ is relatively large
- sometimes $\dim(x)$ is also relatively large compared to $n$
- in addition, machine learning typically requires to estimate several models, several times
- in practice, this requires computing power and takes time

> In the course, we will deal with "not so big" data for practical purposes

## Data

We will begin by a look at consumption data and focus on relations between **total expenses** of a household and the **share of a good** in total consumption. Here we will study food share in SouthAfrica^[Data from A. Yatchew *Semiparametric  Regression for the Applied Econometrician*
https://www.economics.utoronto.ca/yatchew/ ].

```{r cache=TRUE}
# We load the data from the SIAP's server
SouthAfrica <-read.csv2("https://www.unsiap.or.jp/on_line/2025/MLO/SouthAfrica.csv")
```
In this data set there are **`r nrow(SouthAfrica)`** observations and  **`r ncol(SouthAfrica)`** variables. The most important are: 

  - `ltexp` is expenditure in log
  - `FoodShr` is the food share of the household
  - The variables $z_{ij}$ are dummies for families with "i" adults and "j" kids.
 
Now we select **only singles** households and reorder according to expenditure variable.

```{r }
Singles <- SouthAfrica[SouthAfrica$z10==1,4:5]  
#Singles Subset
Singles <- Singles[order(Singles$ltexp),1:2]
```

```{r }
#Reorder so that log expenditure is in increasing order
FoodShr <- Singles$FoodShr 
ltexp   <- Singles$ltexp
MyData <- data.frame(FoodShr,ltexp)
```

The data set of interest is now `MyData` and has  **`r nrow(MyData)`** observations and  **`r ncol(MyData)`** variables.

## Descriptive statistics

Let's first look at the data.

```{r }
hist(ltexp,prob=T,breaks=50, 
     main = "Histogram of log(expenses)",
     sub = "Red curve is the density (estimated)",
     xlab = "log(expenses)" ,
     col = "grey", border = "white")
lines(density(ltexp,na.rm = TRUE),lwd=2,col = "darkred")
```


```{r }
hist(FoodShr,prob=T,breaks=50, 
     main = "Histogram of Food Share",
     sub = "Red curve is the density (estimated)",
     xlab = "Food Share" ,
     col = SIAP.color, border = "white")
lines(density(FoodShr,na.rm = TRUE),lwd=2,col = "darkred")

```


## Visualizing relationships

This is the scatterplot of our observations.

```{r M0-Scatter }
plot(FoodShr~ltexp, main="Scatter plot of Food Share vs Log(exp)", 
     xlab="Log(Exp)",ylab = "FoodShare",
    pch=19, cex = 0.5,col = "grey", frame.plot = FALSE )
```


# Exporing the relationships: It is all about $f(\cdot)$

We may be interested in the relation between  Food Share (*FoodShr*)  and Expenses (*ltexp*) and thus in the expression 
$$ y = f(x) + \varepsilon$$
where $y$ = *FoodShr* and $x$ = *ltexp*

> In essence, statistical learning refers to a set of approaches for estimating $f(\cdot)$

*James, Witten,Hastie & Tibshirani* (2021)

## How to estimate $f(\cdot)$?


## Linear Model

Let's begin with a simple linear model.
$$y = x'\beta + \varepsilon$$
 When we run a linear model, we have an output like this: 
 
```{r }
# Defining the regression of Y:  FoodShr on X:  ltexp
lmFood <- lm(FoodShr~ltexp)
print(summary(lmFood))
```


 But we can focus on the main relations and coefficients of the regression 
 $$ FooShr =  \beta_o +\beta1 * ltexp + \varepsilon$$
 
```{r}
# We use the package boroom and function tidy (broom::tidy) to have results printed nicely 

tidy(lmFood) %>%
  kable(digits = 2, format = "simple", caption = "Regression Summary")
```

## Regression model

Visualizing the estimated regression line. Note how it is constructed in practice.


```{r Scatter-lm}
# Fit the linear model
lmFood <- lm(FoodShr ~ ltexp, data = Singles)

# Create predictions with confidence intervals
lmpredictions <- Singles %>%
  mutate(pred = predict(lmFood, newdata = Singles)) 

# Plot with ggplot2
ggplot(Singles, aes(x = ltexp, y = FoodShr)) +
  geom_point(color = "grey", size = 1, alpha = 0.5) +  # Scatter plot
  geom_line(aes(y = lmpredictions$pred), color = "blue", size = 0.7) +  # Regression line
  labs(title = "Regression on the data set",
       x = "Log(Exp)", y = "FoodShare") +
  theme_minimal()

```
We can also have a confidence interval around the linear regression

```{r Scatter-lm-conf}
# Fit the linear model
lmFood <- lm(FoodShr ~ ltexp, data = Singles)

# Create predictions with confidence intervals
lmpredconf <- Singles %>%
  mutate(pred = predict(lmFood, newdata = Singles, interval = "confidence")) %>%
  bind_cols(as.data.frame(predict(lmFood, newdata = Singles, interval = "confidence")))

# Plot with ggplot2
ggplot(Singles, aes(x = ltexp, y = FoodShr)) +
  geom_point(color = "grey", size = 1, alpha = 0.5) +  # Scatter plot
  geom_line(aes(y = lmpredconf$fit), color = "blue", size = 0.7) +  # Regression line
  geom_ribbon(aes(ymin = lmpredconf$lwr, ymax = lmpredconf$upr), 
              fill = "blue", alpha = 0.2) +  # Confidence interval
  labs(title = "Regression with Confidence Interval",
       x = "Log(Exp)", y = "FoodShare") +
  theme_minimal()

```

## How is this regression line computed? 

You may recall that the regression is obtained by trying to find the line defined by the equation $\beta_0 + \beta_1 x$ that *fits* the data and *minimize the vertical distance between a point and the estimated line*. In other words,  we are looking for  $\beta_0$ and $\beta_1$, the *parameters* of the line, such that the sum of all distances for all points is minimized. The vertical distance of any $y_i$ to the line for a particular point $i$ is simply:

$$ \bigl(y_i - (\beta_0 + \beta_1 x_i)\bigr)^2$$

```{r M0-scatter-Regline}
# Setting the plot with points
plot(ltexp, FoodShr, type="n",
     main="Linear regression", 
     sub = "In red,  the distance to the regression line for some observations",
     xlab="Log(Exp)", ylab = "FoodShare",
    pch=19, cex = 0.5,col = "grey", frame.plot = FALSE )
points(ltexp,FoodShr,
    pch=19, cex = 0.5,col = "grey" )

# Estimation of the linear regression model 
lmFood <- lm(FoodShr ~ poly(ltexp, degree=1,raw=TRUE))

# Plotting the regression line for a sequence of points  
newx <- seq(from=min(ltexp),to=max(ltexp),
            length.out = 200)
lines(newx, predict(lmFood, data.frame(ltexp = newx)),
      col = "blue")

# Plotting the vertical distances in red
i <- 6               # Here we take the distance for the 6th point
segments(ltexp[i] , FoodShr[i], ltexp[i], lmFood$coefficients[1] + lmFood$coefficients[2]* ltexp[i]
         , col = "red", lw=2)
# Vertical distances for some other points
segments(ltexp[555] , FoodShr[555], ltexp[555],
         lmFood$coefficients[1] + lmFood$coefficients[2]* ltexp[555],
         col = "red", lw=2)
segments(ltexp[725] , FoodShr[725], ltexp[725],
         lmFood$coefficients[1] + lmFood$coefficients[2]* ltexp[725],
         col = "red", lw=2)

```


Therefore, the regression line can be found by minimizing the *residual sum of squares* ($RSS$, see below) that is by solving the following optimization problem:

- find $\beta_0$ and $\beta_1$ such that:
$$Min_{\; (\beta_0 , \beta_1)} \;  \; \; \sum_{i=1}^{n} \bigl(y_i - (\beta_0 + \beta_1 x_i)\bigr)^2$$
is **minimal**. 

> The regression line is found as a solution of an **optimization** problem

In this case when $f(\cdot)$ is linear, an analytical solution exist (the equation can be written explicitly). 



```{r include=FALSE}
# knitr::knit_exit() 
```


### Goodness of fit: $R^2$

We can compute one of the favorite measures of adjustment: the **$R^2$** that measures how close the data are to the fitted regression line. We use the sum of squared distances of the observations $Y_i$ to the regression line, or *residual sum of squares* ($RSS$), as compared to the  *total sum of the squares* ($TSS$,  measured as  the sum of the distances of the observations $Y_i$ to their mean. So:

$$TSS= \sum_{i=1}^n (y_i -\bar{y})^2 $$
and 
$$RSS= \sum_{i=1}^n (y_i - \widehat{f}(x_i))^2 $$

The definition of R-squared is then: $R^2 = \frac{TSS- RSS}{TSS}$ It is simply the explained (by the regression) variation of the outcome variable $y$ divided by the total variation of the outcome variable. It can be noted that $R^2 = 1-  \frac{RSS}{TSS}$ and that the goodness of fit is perfect when equal to 1. 



```{r}
summary(lmFood)$adj.r.squared
```

### Observing the residuals

```{r}
lmFood.res = resid(lmFood)
# We now plot the residual against the observed values of the variable FoodShr.
hist(lmFood.res,prob=T,breaks=50, 
     main = "Histogram of the residuals (Linear model)",
     sub = "Red curve is the density (estimated)",
     xlab = "Food Share" , xlim=c(-1,1),
     col = SIAP.color, border = "white")
lines(density(lmFood.res,na.rm = TRUE),lwd=2,col = "darkred")

```


## Polynomial Models

We can try to have a better model by introducing some non linearity. This can be done using polynomials of the unique regressor $x$. Here we define a polynomial model of order 2, or *quadratic model*:

$$y = \beta_0 + \beta_1 x +  \beta_2 x^2+ \varepsilon$$.

```{r}
# Fit the model
lmFood2 <- lm(FoodShr ~ poly(ltexp, degree = 2, raw = TRUE), data = Singles)
```

Examining the results
```{r}
tidy(lmFood2) %>%
  kable(digits = 2, format = "simple", caption = "Polynomial Regression Summary")
```

Let us see if the adjustment, in terms of the $R^2$ has been better: 

```{r}
summary(lmFood2)$adj.r.squared
```

```{r}
# Create predictions with confidence intervals
predictions2 <- Singles %>%
  mutate(pred = predict(lmFood2, newdata = Singles, interval = "confidence")) %>%
  bind_cols(as.data.frame(predict(lmFood2, newdata = Singles, interval = "confidence")))

# Plot with ggplot2
ggplot(Singles, aes(x = ltexp, y = FoodShr)) +
  geom_point(color = "grey", size = 1, alpha = 0.5) +  # Scatter plot
  geom_line(aes(y = predictions2$fit), color = "blue", size = 0.7) +  # Regression line
  geom_ribbon(aes(ymin = predictions2$lwr, ymax = predictions2$upr), 
              fill = "blue", alpha = 0.2) +  # Confidence interval
  labs(title = "Polynomial Regression with Confidence Interval",
       x = "Log(Exp)", y = "FoodShare") +
  theme_minimal()

```




```{r}
# Gathering residual values
lmFood2.res = resid(lmFood2)

# We now plot the residuals at the observed values of the variable FoodShr.
hist(lmFood2.res,prob=T,breaks=50, 
     main = "Histogram of the residuals (Quadratic model)",
     sub = "Red curve is the density (estimated)",
     xlab = "Food Share" , xlim=c(-1,1),
     col = SIAP.color, border = "white")
lines(density(lmFood2.res,na.rm = TRUE),lwd=2,col = "darkred")

```

#### Let's try with more degrees in our polynomial model (*Cubic model*):

$$y = \beta_0 + \beta_1 x +  \beta_2 x^2+ \beta_3 x^3 + \varepsilon $$.

```{r }
lmFood3 <- lm(FoodShr ~ poly(ltexp, degree=3,raw=TRUE))

tidy(lmFood3) %>%
  kable(digits = 2, format = "simple", caption = "Cubic Polynomila Regression Summary")
```


```{r}
summary(lmFood3)$adj.r.squared
```


```{r}
# Create predictions with confidence intervals
predictions3 <- Singles %>%
  mutate(pred = predict(lmFood3, newdata = Singles, interval = "confidence")) %>%
  bind_cols(as.data.frame(predict(lmFood3, newdata = Singles, interval = "confidence")))

# Plot with ggplot2
ggplot(Singles, aes(x = ltexp, y = FoodShr)) +
  geom_point(color = "grey", size = 1, alpha = 0.5) +  # Scatter plot
  geom_line(aes(y = predictions3$fit), color = "blue", size = 0.7) +  # Regression line
  geom_ribbon(aes(ymin = predictions3$lwr, ymax = predictions3$upr), 
              fill = "blue", alpha = 0.2) +  # Confidence interval
  labs(title = " Cubic Polynomial Regression with Confidence Interval",
       x = "Log(Exp)", y = "FoodShare") +
  theme_minimal()

```


```{r}
# Gathering residual values
lmFood3.res = resid(lmFood3)

# We now plot the residual against the observed values of the variable FoodShr.
hist(lmFood3.res,prob=T,breaks=50, 
     main = "Histogram of the residuals (Cubic model)",
     sub = "Red curve is the density (estimated)",
     xlab = "Food Share" , xlim=c(-1,1),
     col = SIAP.color, border = "white")
lines(density(lmFood2.res,na.rm = TRUE),lwd=2,col = "darkred")

```

## Comparing the residuals distribution

```{r}
# Gathering all residuals
res.all <- as.data.frame(cbind(lmFood.res, lmFood2.res, lmFood3.res))

# Ploting residuals 
boxplot(res.all, 
        ylab ="Residuals", 
        xlab = "Models",
        ylim = c(-0.5, 0.5),
       # outline=FALSE,   
        frame.plot = FALSE,
        horizontal = FALSE,
        col = SIAP.color,
        names = c("Linear", "Quadratic", "Cubic") )

```

# Takeaways
- This is how Data Science works!
- Data Science requires to **test different models**, rerun code several times
- Finding the "best model" depends on **which criterion** is used. This is not so easy. 
- To estimate regression models, we have used **all observations** in the data set. They may be other options ;-).
- With "big data", we can go over a simple linear models like polynomial models (or nonparametric mode such as *k-NN* regression).
- Any "**good criterion**  will depend on the final goal: Inference (like here) or prediction (like in Machine Learning).


```{r include=FALSE}
knitr::knit_exit()
```




