---
title: "Statistical Learning"
author: "Christophe Bontemps & Patrick Jonsson - SIAP"
subtitle: "Workshop on Machine Learning for Official Statistics in Asia-Pacific"
output:
  pdf_document:
    df_print: kable
    toc: true
    keep_tex: true
    fig_width: 6.5
    fig_height: 4
  word_document:
    toc: true
  html_document:
    df_print: paged
    toc: true
    keep_md: true
    code_folding: show
    toc_float: true
    fig_width: 6.5
    fig_height: 4
always_allow_html: true
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Knitr_Global_Options, include=FALSE, cache=FALSE}
library(knitr)

opts_chunk$set(warning = FALSE, message = FALSE, 
               autodep = TRUE, tidy = FALSE, cache = TRUE)
#opts_chunk$set(cache.rebuild=TRUE) 

# My colors:
SIAP.color <- "#0385a8"

```

`r if(knitr:::pandoc_to() == "latex") {paste("\\large")}` 

## Tools

We will use several packages to make our lives easier. Here is the list of packages we will use: 

```{r}
# Data Science packages
library(tidyverse)
library(jtools)

# Pretty tables
library(broom)
library(xtable)
library(kableExtra)

```

# Introduction
This file serves as support for the "*Workshop on Machine Learning for Official Statistics in Asia-Pacific*"  and showcase a typical work stream in Data Science.   


The term *machine learning* is used because the computer (more accurately the algorithm) figures out the model from the data and will then predict with accuracy. On the other hand,  *statistical learning* is mostly used to test whether a model fits the data and to highlight relationships between variables. 

> Let's start with a Data Science example

One specific feature is that in most cases the data is  "big":

- usually $n$ is relatively large
- sometimes $\dim(x)$ is also relatively large compared to $n$
- in addition, machine learning typically requires to estimate several models, several times
- in practice, this requires computing power and takes time

> In the course, we will deal with "not so big" data for practical purposes

## Data

We will begin by a look at consumption data and focus on relations between **total expenses** of a household and the **share of a good** in total consumption. Here we will study food share in SouthAfrica^[Data from A. Yatchew *Semiparametric  Regression for the Applied Econometrician*
https://www.economics.utoronto.ca/yatchew/ ].

```{r cache=TRUE}
# We load the data from the SIAP's server
SouthAfrica <-read.csv2("https://www.unsiap.or.jp/on_line/2025/MLO/SouthAfrica.csv")
```
In this data set there are **`r nrow(SouthAfrica)`** observations and  **`r ncol(SouthAfrica)`** variables. The most important are: 

  - `ltexp` is expenditure in log
  - `FoodShr` is the food share of the household
  - The variables $z_{ij}$ are dummies for families with "i" adults and "j" kids.
 
Now we select **only singles** households and reorder according to expenditure variable.

```{r }
Singles <- SouthAfrica[SouthAfrica$z10==1,4:5]  
#Singles Subset
Singles <- Singles[order(Singles$ltexp),1:2]
```

```{r }
#Reorder so that log expenditure is in increasing order
FoodShr <- Singles$FoodShr 
ltexp   <- Singles$ltexp
MyData <- data.frame(FoodShr,ltexp)
```

The data set of interest is now `MyData` and has  **`r nrow(MyData)`** observations and  **`r ncol(MyData)`** variables.

## Descriptive statistics

Let's first look at the data.

```{r }
hist(ltexp,prob=T,breaks=50, 
     main = "Histogram of log(expenses)",
     sub = "Red curve is the density (estimated)",
     xlab = "log(expenses)" ,
     col = "grey", border = "white")
lines(density(ltexp,na.rm = TRUE),lwd=2,col = "darkred")
```


```{r }
hist(FoodShr,prob=T,breaks=50, 
     main = "Histogram of Food Share",
     sub = "Red curve is the density (estimated)",
     xlab = "Food Share" ,
     col = SIAP.color, border = "white")
lines(density(FoodShr,na.rm = TRUE),lwd=2,col = "darkred")

```


## Visualizing relationships

This is the scatterplot of our observations.

```{r M0-Scatter }
plot(FoodShr~ltexp, main="Scatter plot of Food Share vs Log(exp)", 
     xlab="Log(Exp)",ylab = "FoodShare",
    pch=19, cex = 0.5,col = "grey", frame.plot = FALSE )
```


# Exporing the relationships: It is all about $f(\cdot)$

We may be interested in the relation between  Food Share (*FoodShr*)  and Expenses (*ltexp*) and thus in the expression 
$$ y = f(x) + \varepsilon$$
where $y$ = *FoodShr* and $x$ = *ltexp*

> In essence, statistical learning refers to a set of approaches for estimating $f(\cdot)$

*James, Witten,Hastie & Tibshirani* (2021)

## How to estimate $f(\cdot)$?


## Linear Model

Let's begin with a simple linear model.
$$y = x'\beta + \varepsilon$$
 When we run a linear model, we have an output like this: 
 
```{r }
# Defining the regression of Y:  FoodShr on X:  ltexp
lmFood <- lm(FoodShr~ltexp)
print(summary(lmFood))
```


 But we can focus on the main relations and coefficients of the regression 
 $$ FooShr =  \beta_o +\beta1 * ltexp + \varepsilon$$
 
```{r}
# We use the package boroom and function tidy (broom::tidy) to have results printed nicely 

tidy(lmFood) %>%
  kable(digits = 2, format = "simple", caption = "Regression Summary")
```

## Regression model

Visualizing the estimated regression line. Note how it is constructed in practice.


```{r Scatter-lm}
# Fit the linear model
lmFood <- lm(FoodShr ~ ltexp, data = Singles)

# Create predictions with confidence intervals
lmpredictions <- Singles %>%
  mutate(pred = predict(lmFood, newdata = Singles)) 

# Plot with ggplot2
ggplot(Singles, aes(x = ltexp, y = FoodShr)) +
  geom_point(color = "grey", size = 1, alpha = 0.5) +  # Scatter plot
  geom_line(aes(y = lmpredictions$pred), color = "blue", size = 0.7) +  # Regression line
  labs(title = "Regression on the data set",
       x = "Log(Exp)", y = "FoodShare") +
  theme_minimal()

```
We can also have a confidence interval around the linear regression

```{r Scatter-lm-conf}
# Fit the linear model
lmFood <- lm(FoodShr ~ ltexp, data = Singles)

# Create predictions with confidence intervals
lmpredconf <- Singles %>%
  mutate(pred = predict(lmFood, newdata = Singles, interval = "confidence")) %>%
  bind_cols(as.data.frame(predict(lmFood, newdata = Singles, interval = "confidence")))

# Plot with ggplot2
ggplot(Singles, aes(x = ltexp, y = FoodShr)) +
  geom_point(color = "grey", size = 1, alpha = 0.5) +  # Scatter plot
  geom_line(aes(y = lmpredconf$fit), color = "blue", size = 0.7) +  # Regression line
  geom_ribbon(aes(ymin = lmpredconf$lwr, ymax = lmpredconf$upr), 
              fill = "blue", alpha = 0.2) +  # Confidence interval
  labs(title = "Regression with Confidence Interval",
       x = "Log(Exp)", y = "FoodShare") +
  theme_minimal()

```

## How is this regression line computed? 

You may recall that the regression is obtained by trying to find the line defined by the equation $\beta_0 + \beta_1 x$ that *fits* the data and *minimize the vertical distance between a point and the estimated line*. In other words,  we are looking for  $\beta_0$ and $\beta_1$, the *parameters* of the line, such that the sum of all distances for all points is minimized. The vertical distance of any $y_i$ to the line for a particular point $i$ is simply:

$$ \bigl(y_i - (\beta_0 + \beta_1 x_i)\bigr)^2$$

```{r M0-scatter-Regline}
# Setting the plot with points
plot(ltexp, FoodShr, type="n",
     main="Linear regression", 
     sub = "In red,  the distance to the regression line for some observations",
     xlab="Log(Exp)", ylab = "FoodShare",
    pch=19, cex = 0.5,col = "grey", frame.plot = FALSE )
points(ltexp,FoodShr,
    pch=19, cex = 0.5,col = "grey" )

# Estimation of the linear regression model 
lmFood <- lm(FoodShr ~ poly(ltexp, degree=1,raw=TRUE))

# Plotting the regression line for a sequence of points  
newx <- seq(from=min(ltexp),to=max(ltexp),
            length.out = 200)
lines(newx, predict(lmFood, data.frame(ltexp = newx)),
      col = "blue")

# Plotting the vertical distances in red
i <- 6               # Here we take the distance for the 6th point
segments(ltexp[i] , FoodShr[i], ltexp[i], lmFood$coefficients[1] + lmFood$coefficients[2]* ltexp[i]
         , col = "red", lw=2)
# Vertical distances for some other points
segments(ltexp[555] , FoodShr[555], ltexp[555],
         lmFood$coefficients[1] + lmFood$coefficients[2]* ltexp[555],
         col = "red", lw=2)
segments(ltexp[725] , FoodShr[725], ltexp[725],
         lmFood$coefficients[1] + lmFood$coefficients[2]* ltexp[725],
         col = "red", lw=2)

```


Therefore, the regression line can be found by minimizing the *residual sum of squares* ($RSS$, see below) that is by solving the following optimization problem:

- find $\beta_0$ and $\beta_1$ such that:
$$Min_{\; (\beta_0 , \beta_1)} \;  \; \; \sum_{i=1}^{n} \bigl(y_i - (\beta_0 + \beta_1 x_i)\bigr)^2$$
is **minimal**. 

> The regression line is found as a solution of an **optimization** problem

In this case when $f(\cdot)$ is linear, an analytical solution exist (the equation can be written explicitly). 



```{r include=FALSE}
# knitr::knit_exit() 
```


### Goodness of fit: $R^2$

We can compute one of the favorite measures of adjustment: the **$R^2$** that measures how close the data are to the fitted regression line. We use the sum of squared distances of the observations $Y_i$ to the regression line, or *residual sum of squares* ($RSS$), as compared to the  *total sum of the squares* ($TSS$,  measured as  the sum of the distances of the observations $Y_i$ to their mean. So:

$$TSS= \sum_{i=1}^n (y_i -\bar{y})^2 $$
and 
$$RSS= \sum_{i=1}^n (y_i - \widehat{f}(x_i))^2 $$

The definition of R-squared is then: $R^2 = \frac{TSS- RSS}{TSS}$ It is simply the explained (by the regression) variation of the outcome variable $y$ divided by the total variation of the outcome variable. It can be noted that $R^2 = 1-  \frac{RSS}{TSS}$ and that the goodness of fit is perfect when equal to 1. 



```{r}
summary(lmFood)$adj.r.squared
```

### Observing the residuals

```{r}
lmFood.res = resid(lmFood)
# We now plot the residual against the observed values of the variable FoodShr.
hist(lmFood.res,prob=T,breaks=50, 
     main = "Histogram of the residuals (Linear model)",
     sub = "Red curve is the density (estimated)",
     xlab = "Food Share" , xlim=c(-1,1),
     col = SIAP.color, border = "white")
lines(density(lmFood.res,na.rm = TRUE),lwd=2,col = "darkred")

```


## Polynomial Models

We can try to have a better model by introducing some non linearity. This can be done using polynomials of the unique regressor $x$. Here we define a polynomial model of order 2, or *quadratic model*:

$$y = \beta_0 + \beta_1 x +  \beta_2 x^2+ \varepsilon$$.

```{r}
# Fit the model
lmFood2 <- lm(FoodShr ~ poly(ltexp, degree = 2, raw = TRUE), data = Singles)
```

Examining the results
```{r}
tidy(lmFood2) %>%
  kable(digits = 2, format = "simple", caption = "Polynomial Regression Summary")
```

Let us see if the adjustment, in terms of the $R^2$ has been better: 

```{r}
summary(lmFood2)$adj.r.squared
```

```{r}
# Create predictions with confidence intervals
predictions2 <- Singles %>%
  mutate(pred = predict(lmFood2, newdata = Singles, interval = "confidence")) %>%
  bind_cols(as.data.frame(predict(lmFood2, newdata = Singles, interval = "confidence")))

# Plot with ggplot2
ggplot(Singles, aes(x = ltexp, y = FoodShr)) +
  geom_point(color = "grey", size = 1, alpha = 0.5) +  # Scatter plot
  geom_line(aes(y = predictions2$fit), color = "blue", size = 0.7) +  # Regression line
  geom_ribbon(aes(ymin = predictions2$lwr, ymax = predictions2$upr), 
              fill = "blue", alpha = 0.2) +  # Confidence interval
  labs(title = "Polynomial Regression with Confidence Interval",
       x = "Log(Exp)", y = "FoodShare") +
  theme_minimal()

```




```{r}
# Gathering residual values
lmFood2.res = resid(lmFood2)

# We now plot the residuals at the observed values of the variable FoodShr.
hist(lmFood2.res,prob=T,breaks=50, 
     main = "Histogram of the residuals (Quadratic model)",
     sub = "Red curve is the density (estimated)",
     xlab = "Food Share" , xlim=c(-1,1),
     col = SIAP.color, border = "white")
lines(density(lmFood2.res,na.rm = TRUE),lwd=2,col = "darkred")

```

#### Let's try with more degrees in our polynomial model (*Cubic model*):

$$y = \beta_0 + \beta_1 x +  \beta_2 x^2+ \beta_3 x^3 + \varepsilon $$.

```{r }
lmFood3 <- lm(FoodShr ~ poly(ltexp, degree=3,raw=TRUE))

tidy(lmFood3) %>%
  kable(digits = 2, format = "simple", caption = "Cubic Polynomila Regression Summary")
```


```{r}
summary(lmFood3)$adj.r.squared
```


```{r}
# Create predictions with confidence intervals
predictions3 <- Singles %>%
  mutate(pred = predict(lmFood3, newdata = Singles, interval = "confidence")) %>%
  bind_cols(as.data.frame(predict(lmFood3, newdata = Singles, interval = "confidence")))

# Plot with ggplot2
ggplot(Singles, aes(x = ltexp, y = FoodShr)) +
  geom_point(color = "grey", size = 1, alpha = 0.5) +  # Scatter plot
  geom_line(aes(y = predictions3$fit), color = "blue", size = 0.7) +  # Regression line
  geom_ribbon(aes(ymin = predictions3$lwr, ymax = predictions3$upr), 
              fill = "blue", alpha = 0.2) +  # Confidence interval
  labs(title = " Cubic Polynomial Regression with Confidence Interval",
       x = "Log(Exp)", y = "FoodShare") +
  theme_minimal()

```


```{r}
# Gathering residual values
lmFood3.res = resid(lmFood3)

# We now plot the residual against the observed values of the variable FoodShr.
hist(lmFood3.res,prob=T,breaks=50, 
     main = "Histogram of the residuals (Cubic model)",
     sub = "Red curve is the density (estimated)",
     xlab = "Food Share" , xlim=c(-1,1),
     col = SIAP.color, border = "white")
lines(density(lmFood2.res,na.rm = TRUE),lwd=2,col = "darkred")

```

## Comparing the residuals distribution

```{r}
# Gathering all residuals
res.all <- as.data.frame(cbind(lmFood.res, lmFood2.res, lmFood3.res))

# Ploting residuals 
boxplot(res.all, 
        ylab ="Residuals", 
        xlab = "Models",
        ylim = c(-0.5, 0.5),
       # outline=FALSE,   
        frame.plot = FALSE,
        horizontal = FALSE,
        col = SIAP.color,
        names = c("Linear", "Quadratic", "Cubic") )

```


# Takeaways
- This is how Data Science works!
- Data Science requires to **test different models**, rerun code several times
- Finding the "best model" depends on **which criterion** is used. This is not so easy. 
- To estimate regression models, we have used **all observations** in the data set. They may be other options ;-).
- With "big data", we can go over a simple linear models like polynomial models (or nonparametric mode such as *k-NN* regression).
- Any "**good criterion**  will depend on the final goal: Inference (like here) or prediction (like in Machine Learning).


```{r include=FALSE}
knitr::knit_exit()
```



## Nonparametric models
### Nearest-Neighbors (k-NN)

We assume that $f(\cdot)$ is *smooth*

* No jumps: continuous
* No kinks: differentiable
* Smooth enough: usually twice differentiable

We want to estimate $f(\cdot)$. We talk about

* **nonparametric regression**, since there is no parameter to be estimated.
* **functional estimation**, since we estimate a function.

Note that we could find a function $f(\cdot)$ that goes through every observation: this is called *interpolation*. There is actually an infinity of such functions, these are defined uniquely only at observations points.

*Nearest-neighbors method* is close to *moving average*: we estimate $f(x)$ by averaging the $y_i$ corresponding to observations $x_i$ close to $x$. That's the idea of *smoothing*.
Since the $x_i$ are ordered from smallest to largest. We define the estimate of $f(x_i)$ as
\[
\widehat{f} (x_i) = \frac{1}{k} \sum_{j \in \, k-nearest\, neighbours \, of \, x_i} y_j
\]
$k$ is the number of neighbors of $x_i$ taken into account in estimation. This method is  called *k-nearest neighbors* (K-NN for short).

**Note**: Our estimator should be defined at any point $x$, even if $x$ is not an observation, so
\[
\widehat{f} (x) = \widehat{f} (x_i)
\]
where $x_i$ is the closest point to $x$.
So we obtain a step function or *piecewise constant* function.

### Playing with $k$

Here we have `r length(ltexp)` observations in our dataset. We can choose different values for $k$ and see how this affect our estimation of $f(\cdot)$: 


> Experiment with different number of nearest-neighbors. What do you get for a small number $k$? What happens when you increase $k$? Use the [online Shiny application to play with k-nn ](https://xtophedataviz.shinyapps.io/KnnExplore/)


```{r, cache = FALSE}
library(caret)
# Change the value of k here!!
k.choice = 250
```

#### k = `r k.choice`    (*try with k = 10, 50, 100, 400, or 1000*)


```{r M0-knn-kchoice-points, cache = FALSE  }

# Scatter plot
plot(ltexp,FoodShr,type="n",
     main= paste("K-NN regression with k=", k.choice,""),
     sub = "Points used for a specific x", 
     xlab="Log(Exp)", ylab = "FoodShare",
    pch=19, cex = 0.5,col = "grey", frame.plot = FALSE )
points(ltexp,FoodShr,
    pch=19, cex = 0.5,col = "grey" )

library(tidyverse)
# for a specific x, highlight the points included in computation
my.index <- 200  #  <-- value can be changed here
my.x <- ltexp[my.index]
my.y <- FoodShr[my.index]


# computing x's nearest neighbors
df <- as.data.frame(cbind(ltexp, FoodShr))
df <- df %>%
  mutate( dist = abs(ltexp - my.x) ) %>%
  arrange(dist) %>%
  slice(1:k.choice)

points(df$ltexp,df$FoodShr,
    pch=19, cex = 0.6,col = "pink" )
# Original values
points(my.x,0,
    pch=15, cex = 0.6,col = "red" )

points(my.x,my.y,
    pch=18, cex = 0.9,col = "black" )


```


```{r M0-knn-kchoice-points-y, cache = FALSE  }
plot(ltexp,FoodShr,type="n",
     main= paste("K-NN regression with k=", k.choice,""),
     xlab="Log(Exp)", ylab = "FoodShare",
    pch=19, cex = 0.5,col = "grey", frame.plot = FALSE )
points(ltexp,FoodShr,
    pch=19, cex = 0.5,col = "grey" )


# Plotting x's nearest neighbors
points(df$ltexp,df$FoodShr,
    pch=19, cex = 0.6,col = "pink" )

# Computing estimation of Y using x's nearest neighbors
my.y.hat <- mean(df$FoodShr)
segments(0, my.y.hat, my.x, my.y.hat,
         lw = 2, 
         col= 'pink')

# Original values
points(my.x,0,
    pch=15, cex = 0.6,col = "red" )

points(my.x,my.y,
    pch=18, cex = 0.9,col = "black" )

# Some illustration on the graphic
segments( my.x, 0, my.x, my.y.hat,
         lw = 2, 
         col= 'pink')
points(my.x,my.y.hat,
    pch=15, cex = 0.6,col = "red" )
rug(df$FoodShr, side=2, col = "pink")

points(0,my.y.hat,
    pch=15, cex = 0.6,col = "red" )


```


```{r ekgeneric, cache = FALSE }
#Estimating Food Shares using k-NN (CARET package)
knn.est <- knnreg(FoodShr~ltexp, data = MyData, k = k.choice)
```

```{r M0-knn-kchoice-line, cache = FALSE  }
plot(ltexp,FoodShr,type="n",
     main= paste("K-NN regression with k=", k.choice,""),
     xlab="Log(Exp)", ylab = "FoodShare",
    pch=19, cex = 0.5,col = "grey", frame.plot = FALSE )
points(ltexp,FoodShr,
    pch=19, cex = 0.5,col = "grey" )

# Defining the sequence of 200 points where we will estimate the k-NN line
newx <- seq(from=min(ltexp),to=max(ltexp),
            length.out = 200)
# Estimating the k-NN regression line
newy <-predict(knn.est, data.frame(ltexp = newx))
# plotting the k-NN regression line
lines(newx, newy, col= "blue")
```




```{r M0-knn-kchoice-full, cache = FALSE  }
plot(ltexp,FoodShr,type="n",
     main= paste("K-NN regression with k=", k.choice,""),
     xlab="Log(Exp)", ylab = "FoodShare",
    pch=19, cex = 0.5,col = "grey", frame.plot = FALSE )
points(ltexp,FoodShr,
    pch=19, cex = 0.5,col = "grey" )

# Defining the sequence of 200 points where we will estimate the k-NN line
newx <- seq(from=min(ltexp),to=max(ltexp),
            length.out = 200)
# Estimating the k-NN regression line
newy <-predict(knn.est, data.frame(ltexp = newx))
# plotting the k-NN regression line
lines(newx, newy, col= "blue")


# Plotting x's nearest neighbors
points(df$ltexp,df$FoodShr,
    pch=19, cex = 0.6,col = "pink" )

# Original values
points(my.x,0,
    pch=15, cex = 0.6,col = "red" )

points(my.x,my.y,
    pch=18, cex = 0.9,col = "black" )

# Some illustration on the graphic
segments( my.x, 0, my.x, my.y.hat,
         lw = 2, 
         col= 'pink')
points(my.x,my.y.hat,
    pch=15, cex = 0.6,col = "red" )
rug(df$FoodShr, side=2, col = "pink")

points(0,my.y.hat,
    pch=15, cex = 0.6,col = "red" )




```



### Residuals distribution


```{r M0-knnhist-kchoice, cache = FALSE}
# Computing the predictions for the observed x_i
yhat <-predict(knn.est, data.frame(ltexp))
knn.250.res <- yhat - FoodShr

# We now plot the residual against the observed values of the variable FoodShr.
hist(knn.250.res,prob=T,breaks=50, 
     main = paste("Histogram of the residuals K-nn ( k=",k.choice,")") ,
     sub = "Red curve is the density (estimated)",
     xlab = "Food Share" , xlim=c(-1,1),
     col = SIAP.color, border = "white")
lines(density(lmFood2.res,na.rm = TRUE),lwd=2,col = "darkred")

```



#### k = 10

```{r ek0, cache = FALSE}
k.choice = 10
knn.est <- knnreg(FoodShr~ltexp, data = MyData, k = k.choice)
```

```{r M0-knn-10, cache = FALSE}

plot(ltexp,FoodShr,type="n",
     main= paste("K-NN regression with k=", k.choice,""),
     xlab="Log(Exp)", ylab = "FoodShare",
    pch=19, cex = 0.5,col = "grey", frame.plot = FALSE )
points(ltexp,FoodShr,
    pch=19, cex = 0.5,col = "grey" )

# Defining the sequence of 200 points where we will estimate the k-NN line
newx <- seq(from=min(ltexp),to=max(ltexp),
            length.out = 200)
# Estimating the k-NN regression line
newy <-predict(knn.est, data.frame(ltexp = newx))
# plotting the k-NN regression line
lines(newx, newy, col= "blue")
```

### Residuals distribution

```{r M0-knnhist-10, cache = FALSE}
# Computing the predictions for the observed x_i
yhat <-predict(knn.est, data.frame(ltexp))
knn.10.res <- yhat - FoodShr

# We now plot the residual against the observed values of the variable FoodShr.
hist(knn.10.res,prob=T,breaks=50, 
     main = paste("Histogram of the residuals K-nn ( k=",k.choice,")") ,
     sub = "Red curve is the density (estimated)",
     xlab = "Food Share" , xlim=c(-1,1),
     col = SIAP.color, border = "white")
lines(density(lmFood2.res,na.rm = TRUE),lwd=2,col = "darkred")

```
#### k = 400

```{r ek400, cache = FALSE}
k.choice = 400
knn.est <- knnreg(FoodShr~ltexp, data = MyData, k = k.choice)
```

```{r M0-knn-400 ,cache = FALSE}
plot(ltexp,FoodShr,type="n",
     main= paste("K-NN regression with k=", k.choice,""),
     xlab="Log(Exp)", ylab = "FoodShare",
    pch=19, cex = 0.5,col = "grey", frame.plot = FALSE )
points(ltexp,FoodShr,
    pch=19, cex = 0.5,col = "grey" )

# Defining the sequence of 200 points where we will estimate the k-NN line
newx <- seq(from=min(ltexp),to=max(ltexp),
            length.out = 200)
# Estimating the k-NN regression line
newy <-predict(knn.est, data.frame(ltexp = newx))
# plotting the k-NN regression line
lines(newx, newy, col= "blue")
```

### Residuals distribution

```{r M0-knnhist-400, cache = FALSE}
# Computing the predictions for the observed x_i
yhat <-predict(knn.est, data.frame(ltexp))
knn.400.res <- yhat - FoodShr

# We now plot the residual against the observed values of the variable FoodShr.
hist(knn.400.res,prob=T,breaks=50, 
     main = paste("Histogram of the residuals K-nn ( k=",k.choice,")") ,
     sub = "Red curve is the density (estimated)",
     xlab = "Food Share" , xlim=c(-1,1),
     col = SIAP.color, border = "white")
lines(density(lmFood2.res,na.rm = TRUE),lwd=2,col = "darkred")

```

## Comparing the residuals distribution

```{r M0-knnhist-all}
res.all <- as.data.frame(cbind(knn.10.res, knn.250.res, knn.400.res))
boxplot(res.all, ylim = c(-0.5, 0.5),outline=FALSE,   
        frame.plot = FALSE, horizontal = TRUE, 
        names = c("k-nn (10)", "k-nn (250)", "k-nn (400)") )

```


### Mean Squared Error
*The MSE* is a **theoretical^[One can estimate the MSE on the sample when $y_i$ is observed and then $$MSE =  \frac{1}{n} \; \sum_ {i=1}^n \bigl( y_i - \widehat f(x_i) \bigr)^2 $$]** measure of the precision for any estimator, it is defined as the Expectation of the distance between the estimated $\widehat f(x_i)$ and true (unknown) value $f(x_i)$ for a particular observation $i$: 

$$
E\left[\widehat{f} (x_i) - f (x_i)\right]^{2}
$$

It can be shown that the MSE can be decomposed into 2 terms: 


$$ MSE= E\left[ \bigl(\widehat{f} (x_i) - f (x_i)\bigr)^{2} \right]= 
\left\{E \left[\widehat{f} (x_i) - f(x_i)\right]\right\}^{2}
+
Var \left[ \widehat{f}(x_i)  \right]$$

*The MSE* is a measure of the precision for any estimator, and we always have

$$MSE = Bias^{2} + Variance $$
Since the *true* function $f(\cdot)$ is unknown, we cannot estimate the first term that depends on the *true$ fucntion $f(\cdot)$ (more precisely on its  second derivative) and since we cannot assume we obtain an unbiased estimator, the MSE for our nonparametric k-NN estimator is:
$$
MSE_{K-NN}= E\left[ \widehat{f} (x_i) - f (x_i)\right]^{2} \approx
      \left\{f^{''} (x_i) \frac{1}{24} \left( \frac{k}{n} \right)^{2} \right\}^{2}
+
    \frac{1}{k} \sigma^{2}_{\varepsilon}
\, .
$$

### Bias-Variance Trade-Off

To minimize the MSE, we should balance squared bias and variance. from this expression we can learn that: 
* bias increases when k increases
* variance decreases when k increases
and alos that 
* bias decreases when n increases 

Then , we should choose $k$ such that the squared bias is of the same order than the variance. One can show that the **optimal $k$** is: 
$k^{*} \propto n^{4/5}$



### Under/Over Smoothing

* *Undersmoothing* occurs is when we use too small a $k$.

Think about $k=1$: we have *interpolation*.
More generally undersmoothing occurs when we obtain a very **wiggly** curve: bias is small (we are near each observation), but variance is large (curve is wiggly).

* *Oversmoothing* is when we use too large small $k$.

Think about $k=n$: the estimator $\widehat{f} (x) = \bar{y}$ for any $x$! More generally oversmothing occurs when we obtain too flat a curve: variance is small, but bias is large.

> In practice, it may be tricky to determine the right number of neighbors that is the right amount of smoothing!


# Wrap-up
- ML is about estimating an unknown function $f(\cdot)$ 
- To estimate regression models, we have to solve an **optimization problem**
- With "big data", we can go over a simple linear model: e.g. polynomial models or nonparametric mode such as *k-NN* regression.
- There is a *bias-variance trade-off*: a more complex model allows to estimate the regression more accurately, but introduces more variability in estimation.
- Theory tells us exactly how to balance squared bias and variance but it does not tell us how to choose the model in practice!



